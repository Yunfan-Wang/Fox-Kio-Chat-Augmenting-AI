T-Technical details, B-Business concerns, A-Academic thoughts
1/6/2026 (T) Allow for people to train their own Koi or Fox agent persona from their own chatting history
1/6/2026 (B) We can allow for people to sell their self trained persona on our platform, each user can subscribe for another person's persona
1/6/2026 (T) V1 is complete, Input: context + draft + persona ids, Koi: infers goal, Fox: optimizes replies based on inferred goal. But we can always push this forward to v2, 
Input: context + draft + persona ids + goal_spec (explicit) Koi: uses provided goal and focuses on alignment/progress/drift Fox: optimizes replies specifically to achieve that goal.
1/6/2026 (B) We can build this product to assist the industry to train their employees
1/7/2026 (A) Using multiple agents with each agent specializing on one task helps to improve explanability and together using an orchestrator will jackpot the user needs
1/7/2026 (T) We should later on allow for local deployment for the fastest real-time generation. Furthermore, we can allow for specialized hardwares
1/7/2026 (T) Stepping into v2. For the Koi personas, we should explicitly define "You MUST use the explicit goal spec provided. Do NOT invent or redefine the goal. Your job is to evaluate alignment, drift, missing info, and next step.".
1/8/2026 (A!) To prove superiority, we can test in terms of time takes to generate, accuracy of generation, complexity of prompt formation
1/8/2026 (B) The targeted users are serious communicators
1/8/2026 (T) later on, we can set up a training, or reviewing session function for people to enhance.
1/8/2026 (B) In the business sense, focused specialisation is more attractive.
1/8/2026 (AB!) Many times, people are too sensitive to AI, and sometimes have already generated aesthetic fatigue, would it help to reduce the mentions of AI?
1/8/2026 (B) We need aesthetic workers to improve our project.
1/8/2026 (B) The problem I want to address is that in one hand, AI hasn't really get involved in people's communication decision process which is a loss, the reason is AI's sophistication and people's awkwardness to let someone else even if it's an AI to monitor them. And there was a practical need for enhancing the efficiency of communication (a lot of emotional training camps, business negotiation training camps, even in the worst way, telecom fraud training etc) 
1/8/2026 (B) We don't need an omnipotent model like chatgpt, gemini, we only need a bot specialized for reasoning skills and talk skills.
1/9/2026 (A!) The idea of SLab is to address people's emotional needs without letting people feel the awkwardness brought by machines. The one essence of human emotion is its volatility, one person can feel multiple emotions in a mili second, the driven aggregated emotion are we making personalized decisions. This can be achieved through multi agent collaboration. Collaborations can be done through multiple models, "centralised orchestration (information separation to achieve uniqueness)", "relay collaboration", "deep think (solo modelling)" and... For each model, the best way to mimic is through persona playing.